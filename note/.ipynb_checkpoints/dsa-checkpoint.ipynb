{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version : 2.2.0\n",
      "keras version : 2.3.0-tf\n",
      "tushare version : 1.2.59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tushare as ts\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"tensorflow version : %s\" % tf.__version__)\n",
    "print(\"keras version : %s\" % keras.__version__)\n",
    "print(\"tushare version : %s\" % ts.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-06-02</th>\n",
       "      <td>967.609</td>\n",
       "      <td>997.119</td>\n",
       "      <td>997.12</td>\n",
       "      <td>952.61</td>\n",
       "      <td>1074627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-03</th>\n",
       "      <td>1002.355</td>\n",
       "      <td>998.394</td>\n",
       "      <td>1026.70</td>\n",
       "      <td>997.77</td>\n",
       "      <td>1616805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-04</th>\n",
       "      <td>989.681</td>\n",
       "      <td>1027.681</td>\n",
       "      <td>1027.68</td>\n",
       "      <td>986.50</td>\n",
       "      <td>1500295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-07</th>\n",
       "      <td>1005.028</td>\n",
       "      <td>1069.468</td>\n",
       "      <td>1075.22</td>\n",
       "      <td>1001.70</td>\n",
       "      <td>2655275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-08</th>\n",
       "      <td>1067.324</td>\n",
       "      <td>1067.154</td>\n",
       "      <td>1075.50</td>\n",
       "      <td>1050.96</td>\n",
       "      <td>1976720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>1778.960</td>\n",
       "      <td>1755.810</td>\n",
       "      <td>1780.52</td>\n",
       "      <td>1751.91</td>\n",
       "      <td>12320747.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>1755.140</td>\n",
       "      <td>1758.820</td>\n",
       "      <td>1760.46</td>\n",
       "      <td>1742.26</td>\n",
       "      <td>9869696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>1756.040</td>\n",
       "      <td>1745.240</td>\n",
       "      <td>1760.42</td>\n",
       "      <td>1743.77</td>\n",
       "      <td>10633246.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>1742.470</td>\n",
       "      <td>1745.020</td>\n",
       "      <td>1759.10</td>\n",
       "      <td>1742.02</td>\n",
       "      <td>11093408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>1745.950</td>\n",
       "      <td>1752.650</td>\n",
       "      <td>1753.98</td>\n",
       "      <td>1741.00</td>\n",
       "      <td>10316100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1844 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                open     close     high      low      volume\n",
       "date                                                        \n",
       "2010-06-02   967.609   997.119   997.12   952.61   1074627.0\n",
       "2010-06-03  1002.355   998.394  1026.70   997.77   1616805.0\n",
       "2010-06-04   989.681  1027.681  1027.68   986.50   1500295.0\n",
       "2010-06-07  1005.028  1069.468  1075.22  1001.70   2655275.0\n",
       "2010-06-08  1067.324  1067.154  1075.50  1050.96   1976720.0\n",
       "...              ...       ...      ...      ...         ...\n",
       "2017-12-25  1778.960  1755.810  1780.52  1751.91  12320747.0\n",
       "2017-12-26  1755.140  1758.820  1760.46  1742.26   9869696.0\n",
       "2017-12-27  1756.040  1745.240  1760.42  1743.77  10633246.0\n",
       "2017-12-28  1742.470  1745.020  1759.10  1742.02  11093408.0\n",
       "2017-12-29  1745.950  1752.650  1753.98  1741.00  10316100.0\n",
       "\n",
       "[1844 rows x 5 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cyb.csv\").set_index('date').sort_index()\n",
    "df = df[['open','close','high','low','volume']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-06-02</th>\n",
       "      <td>0.132944</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-03</th>\n",
       "      <td>0.131497</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-04</th>\n",
       "      <td>0.099252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-07</th>\n",
       "      <td>0.056301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-08</th>\n",
       "      <td>0.058591</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1844 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                incr  label\n",
       "date                       \n",
       "2010-06-02  0.132944      1\n",
       "2010-06-03  0.131497      1\n",
       "2010-06-04  0.099252      1\n",
       "2010-06-07  0.056301      1\n",
       "2010-06-08  0.058591      1\n",
       "...              ...    ...\n",
       "2017-12-25       NaN      0\n",
       "2017-12-26       NaN      0\n",
       "2017-12-27       NaN      0\n",
       "2017-12-28       NaN      0\n",
       "2017-12-29       NaN      0\n",
       "\n",
       "[1844 rows x 2 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incr = df['high'].rolling(10).max().shift(-10) / df['close'] - 1\n",
    "df['incr'] = incr\n",
    "df['label'] = incr.apply(lambda x: 1 if x > 0.05 else 0)\n",
    "df = df[['incr', 'label']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:288: UserWarning: Data must have variance to compute a kernel density estimate.\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/seaborn/distributions.py:288: UserWarning: Data must have variance to compute a kernel density estimate.\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x1a4110d890>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 222.375x180 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.pairplot(df[['close', 'incr', 'label']], diag_kind='kde', hue='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>incr</th>\n",
       "      <td>1467.0</td>\n",
       "      <td>0.045142</td>\n",
       "      <td>0.042353</td>\n",
       "      <td>-0.040735</td>\n",
       "      <td>0.012101</td>\n",
       "      <td>0.034179</td>\n",
       "      <td>0.067269</td>\n",
       "      <td>0.258652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count      mean       std       min       25%       50%       75%  \\\n",
       "incr  1467.0  0.045142  0.042353 -0.040735  0.012101  0.034179  0.067269   \n",
       "\n",
       "           max  \n",
       "incr  0.258652  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "train_data = df.sample(frac=0.8,random_state=0)\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "train_stat = train_data.describe()\n",
    "train_stat.pop('label')\n",
    "train_stat = train_stat.transpose()\n",
    "train_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data.pop('label')\n",
    "test_label = test_data.pop('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_52 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 49\n",
      "Trainable params: 49\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 1s 40ms/step - loss: 0.6825 - accuracy: 0.6445 - val_loss: 0.6779 - val_accuracy: 0.5646\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6572 - accuracy: 0.6505 - val_loss: 0.6692 - val_accuracy: 0.5646\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6408 - accuracy: 0.6505 - val_loss: 0.6642 - val_accuracy: 0.5646\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6284 - accuracy: 0.6505 - val_loss: 0.6601 - val_accuracy: 0.5646\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6191 - accuracy: 0.6505 - val_loss: 0.6554 - val_accuracy: 0.5646\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.6107 - accuracy: 0.6505 - val_loss: 0.6492 - val_accuracy: 0.5646\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.6025 - accuracy: 0.6505 - val_loss: 0.6425 - val_accuracy: 0.5646\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.5944 - accuracy: 0.6522 - val_loss: 0.6343 - val_accuracy: 0.5680\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.6556 - val_loss: 0.6243 - val_accuracy: 0.5748\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5773 - accuracy: 0.6607 - val_loss: 0.6152 - val_accuracy: 0.5850\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 7ms/step - loss: 0.5682 - accuracy: 0.6667 - val_loss: 0.6041 - val_accuracy: 0.5986\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.5588 - accuracy: 0.6743 - val_loss: 0.5956 - val_accuracy: 0.6054\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5493 - accuracy: 0.6743 - val_loss: 0.5843 - val_accuracy: 0.6122\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.5390 - accuracy: 0.6982 - val_loss: 0.5695 - val_accuracy: 0.6531\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.5291 - accuracy: 0.7084 - val_loss: 0.5603 - val_accuracy: 0.6565\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5186 - accuracy: 0.7195 - val_loss: 0.5496 - val_accuracy: 0.6599\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.5079 - accuracy: 0.7374 - val_loss: 0.5365 - val_accuracy: 0.6973\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4977 - accuracy: 0.7613 - val_loss: 0.5250 - val_accuracy: 0.7109\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7562 - val_loss: 0.5132 - val_accuracy: 0.7211\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4758 - accuracy: 0.7809 - val_loss: 0.5007 - val_accuracy: 0.7415\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.4652 - accuracy: 0.7860 - val_loss: 0.4892 - val_accuracy: 0.7483\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.7877 - val_loss: 0.4778 - val_accuracy: 0.7687\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.4440 - accuracy: 0.7980 - val_loss: 0.4664 - val_accuracy: 0.7823\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4333 - accuracy: 0.8056 - val_loss: 0.4546 - val_accuracy: 0.8027\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4230 - accuracy: 0.8150 - val_loss: 0.4443 - val_accuracy: 0.8095\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4128 - accuracy: 0.8261 - val_loss: 0.4316 - val_accuracy: 0.8299\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.4029 - accuracy: 0.8303 - val_loss: 0.4211 - val_accuracy: 0.8401\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3929 - accuracy: 0.8372 - val_loss: 0.4102 - val_accuracy: 0.8537\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8551 - val_loss: 0.4007 - val_accuracy: 0.8537\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3741 - accuracy: 0.8474 - val_loss: 0.3894 - val_accuracy: 0.8537\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3649 - accuracy: 0.8670 - val_loss: 0.3814 - val_accuracy: 0.8537\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8576 - val_loss: 0.3716 - val_accuracy: 0.8571\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3481 - accuracy: 0.8883 - val_loss: 0.3624 - val_accuracy: 0.8605\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.3390 - accuracy: 0.8772 - val_loss: 0.3533 - val_accuracy: 0.8639\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3312 - accuracy: 0.8789 - val_loss: 0.3440 - val_accuracy: 0.8776\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.3232 - accuracy: 0.9020 - val_loss: 0.3358 - val_accuracy: 0.8810\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8951 - val_loss: 0.3296 - val_accuracy: 0.8776\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.8994 - val_loss: 0.3194 - val_accuracy: 0.8878\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.9096 - val_loss: 0.3134 - val_accuracy: 0.8878\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2946 - accuracy: 0.9079 - val_loss: 0.3046 - val_accuracy: 0.9014\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2893 - accuracy: 0.9045 - val_loss: 0.2963 - val_accuracy: 0.9048\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2822 - accuracy: 0.9224 - val_loss: 0.2928 - val_accuracy: 0.8946\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2754 - accuracy: 0.9207 - val_loss: 0.2850 - val_accuracy: 0.9014\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2692 - accuracy: 0.9233 - val_loss: 0.2795 - val_accuracy: 0.9014\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2635 - accuracy: 0.9207 - val_loss: 0.2731 - val_accuracy: 0.9048\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2579 - accuracy: 0.9275 - val_loss: 0.2672 - val_accuracy: 0.9048\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2528 - accuracy: 0.9361 - val_loss: 0.2603 - val_accuracy: 0.9184\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.9301 - val_loss: 0.2562 - val_accuracy: 0.9082\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2427 - accuracy: 0.9420 - val_loss: 0.2522 - val_accuracy: 0.9048\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9454 - val_loss: 0.2457 - val_accuracy: 0.9184\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2329 - accuracy: 0.9284 - val_loss: 0.2417 - val_accuracy: 0.9150\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.2288 - accuracy: 0.9488 - val_loss: 0.2370 - val_accuracy: 0.9184\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.2242 - accuracy: 0.9395 - val_loss: 0.2312 - val_accuracy: 0.9286\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.2199 - accuracy: 0.9497 - val_loss: 0.2270 - val_accuracy: 0.9286\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2159 - accuracy: 0.9514 - val_loss: 0.2248 - val_accuracy: 0.9218\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2121 - accuracy: 0.9506 - val_loss: 0.2198 - val_accuracy: 0.9286\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2079 - accuracy: 0.9506 - val_loss: 0.2149 - val_accuracy: 0.9422\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2044 - accuracy: 0.9514 - val_loss: 0.2112 - val_accuracy: 0.9422\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2012 - accuracy: 0.9557 - val_loss: 0.2093 - val_accuracy: 0.9286\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1975 - accuracy: 0.9557 - val_loss: 0.2042 - val_accuracy: 0.9456\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 8ms/step - loss: 0.1944 - accuracy: 0.9540 - val_loss: 0.2000 - val_accuracy: 0.9490\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1915 - accuracy: 0.9557 - val_loss: 0.1967 - val_accuracy: 0.9524\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1878 - accuracy: 0.9557 - val_loss: 0.1944 - val_accuracy: 0.9456\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1851 - accuracy: 0.9599 - val_loss: 0.1911 - val_accuracy: 0.9456\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1824 - accuracy: 0.9574 - val_loss: 0.1885 - val_accuracy: 0.9456\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1791 - accuracy: 0.9531 - val_loss: 0.1844 - val_accuracy: 0.9592\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1763 - accuracy: 0.9642 - val_loss: 0.1822 - val_accuracy: 0.9558\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1737 - accuracy: 0.9582 - val_loss: 0.1789 - val_accuracy: 0.9592\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9616 - val_loss: 0.1769 - val_accuracy: 0.9592\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1682 - accuracy: 0.9616 - val_loss: 0.1742 - val_accuracy: 0.9592\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1659 - accuracy: 0.9633 - val_loss: 0.1719 - val_accuracy: 0.9592\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1634 - accuracy: 0.9625 - val_loss: 0.1696 - val_accuracy: 0.9592\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1613 - accuracy: 0.9693 - val_loss: 0.1668 - val_accuracy: 0.9626\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1591 - accuracy: 0.9591 - val_loss: 0.1647 - val_accuracy: 0.9626\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1566 - accuracy: 0.9693 - val_loss: 0.1624 - val_accuracy: 0.9626\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1546 - accuracy: 0.9650 - val_loss: 0.1610 - val_accuracy: 0.9592\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1525 - accuracy: 0.9650 - val_loss: 0.1576 - val_accuracy: 0.9762\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1508 - accuracy: 0.9770 - val_loss: 0.1569 - val_accuracy: 0.9592\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1486 - accuracy: 0.9608 - val_loss: 0.1540 - val_accuracy: 0.9694\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1466 - accuracy: 0.9719 - val_loss: 0.1516 - val_accuracy: 0.9796\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9719 - val_loss: 0.1501 - val_accuracy: 0.9762\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1428 - accuracy: 0.9744 - val_loss: 0.1488 - val_accuracy: 0.9626\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1414 - accuracy: 0.9676 - val_loss: 0.1464 - val_accuracy: 0.9762\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1395 - accuracy: 0.9795 - val_loss: 0.1448 - val_accuracy: 0.9762\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1376 - accuracy: 0.9702 - val_loss: 0.1438 - val_accuracy: 0.9694\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1361 - accuracy: 0.9736 - val_loss: 0.1419 - val_accuracy: 0.9728\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1345 - accuracy: 0.9710 - val_loss: 0.1401 - val_accuracy: 0.9762\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1331 - accuracy: 0.9804 - val_loss: 0.1391 - val_accuracy: 0.9694\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1316 - accuracy: 0.9710 - val_loss: 0.1371 - val_accuracy: 0.9762\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1301 - accuracy: 0.9719 - val_loss: 0.1357 - val_accuracy: 0.9762\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1288 - accuracy: 0.9787 - val_loss: 0.1339 - val_accuracy: 0.9796\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1270 - accuracy: 0.9778 - val_loss: 0.1322 - val_accuracy: 0.9830\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1256 - accuracy: 0.9761 - val_loss: 0.1308 - val_accuracy: 0.9830\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1244 - accuracy: 0.9847 - val_loss: 0.1297 - val_accuracy: 0.9796\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1230 - accuracy: 0.9770 - val_loss: 0.1282 - val_accuracy: 0.9830\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1218 - accuracy: 0.9736 - val_loss: 0.1264 - val_accuracy: 0.9830\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1209 - accuracy: 0.9872 - val_loss: 0.1267 - val_accuracy: 0.9762\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1191 - accuracy: 0.9787 - val_loss: 0.1246 - val_accuracy: 0.9830\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1183 - accuracy: 0.9795 - val_loss: 0.1234 - val_accuracy: 0.9830\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1173 - accuracy: 0.9889 - val_loss: 0.1214 - val_accuracy: 0.9830\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=[len(train_data.keys())]))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data, train_label, epochs=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fc51a3f187a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
